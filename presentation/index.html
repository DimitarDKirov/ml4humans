<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>ML 4 Humans</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="bower_components/reveal-js/css/reveal.min.css">
		<link rel="stylesheet" href="bower_components/reveal-js/css/theme/beige.css" id ="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="bower_components/reveal-js/lib/css/zenburn.css">

		<!-- If the query includes 'print-pdf', include the PDF print sheet -->
		<script>
			if( window.location.search.match( /print-pdf/gi ) ) {
				var link = document.createElement( 'link' );
				link.rel = 'stylesheet';
				link.type = 'text/css';
				link.href = 'bower_components/reveal-js/css/print/pdf.css';
				document.getElementsByTagName( 'head' )[0].appendChild( link );
			}
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
		<style>
			.container{
			    display: flex;
			}
			.col{
			    flex: 1;
			}
			section {
				text-align: left;
			}
		</style>
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h2>Machine Learning for Humans</h2>
					<p>Interpretability, Interactivity & Bias </p>
					<p> Martin Boyanov <p>
					<p>8th June 2020 - Dev.BG Webinar</p>
				</section>
				<section>
					<h2> Motivation </h2>
					<p> Machine Learning is becoming ever more widespread across various industries. More and more people will work with ML on a daily basis, a lot of them without the relevant education or experience. We owe it to them to provide them with tools to disect the models, understanding their predictions and judging how to change them.

					<p> On the other side of the coin, as people's lives start depending on algorithmical decisions, we need to ensure that said algorithms are legal, fair and unbiased. 
				</section>
				<section data-markdown>
						## Problems

						1. Algorithms are perceived differently - they do not make mistakes, not as much as humans
						2. No appeals process - if you do find a mistake, there is no procedure to fix it
						3. Hiding behind the machine - shifting blame and responsibility to the machine
						4. Feedback loops - ML cares about optimizing metrics; it will not hesitate to exploit biases, stereotypes, injustice to achieve its goal, making them worse in the process
						5. Many, many others...

				</section>
				<section >
					<h2>Why me?</h2>
					<div class="container">
					<div class="col">
						<h3> Professional </h3>
						<ul> 
							<li> Machine Learning @ Smule
							<li> Data Scientist @ Commetric 
						    <li> Data Scientist @ Komfo
						</ul>
					</div>

					<div class="col">
						<h3> Academic </h3>
						<ul>
							<li>MSc @ Sofia University</li>
							<li>BEng @ Edinburgh University</li>
							<li>Competitions, Hackathons, Datathons</li>
						</ul>
					</div>
					</div>

				</section>
				
				<section>
					<h2>Agenda</h2>
					How to achieve interpretability:
					<ol>
						<li> Interpretable models
						<li> Model-agnostic methods
						<li> Example Interpretability for Neural Networks
					</ol>
				</section>
				<section>
					<h2> Credit where credit is due </h2>
					<p> For the technical part, please check out <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning by Christoph Molnar</a>
					<p> For bias & ethics, I recommend <a href="https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction">Weapons of Math Destruction by Cathy O'Neil</a>
					<p> For Deep Learning: <a href="https://arxiv.org/abs/2004.14545"> Explainable Deep Learning: A Field Guide for the Uninitiated</a> </p>
					<p>Also cool: <a href="https://dataskeptic.com/"> Data Skeptic podcast on Intepretability </a>
				</section>
				
				<section>
					
					<section data-markdown>
						## Interpretable models 
						The easiest way to achieve interpretability is to use an interpretable model.

						With *Linear Regression* and *Logistic Regression* we can reason about the weights.    

						With *Decision Trees* we can follow the tree.        

						With *Decision Rules* we can follow the rules.    

						With *Naive Bayes* we can reason about the probabilities.    

						With *K-nearest neighbours* we can show the nearest neighbours.
					</section>
					<section>
						Notebook.
					</section>
				</section>
				<section data-markdown>
					## Model-agnostic methods

					Sometimes, our only means of inspection for a given model is to probe it with different inputs and analyze the outputs.

					We will review the following methods:

					* Partial Dependence Plots
					* Surrogate methods
					* Shapley Values
				</section>

				<section>
					
					<section data-markdown>
						## Example Interpretability for Neural Networks

						Lots of folks claim that ML and specifically DL is a black box. That's fair until you try breaking it up.

					</section>
					
					<section data-markdown>
						## Bias in Embeddings
						The classic examples for word embedding analogies are:

						![](images/manking.png)
						![](images/parisfrance.png)

						Man is to King as Woman is to Queen.       

						Paris is to France as London is to UK.     
						
						Problem: Man is to Doctor as Woman is to ?   	 
						A) Nurse    
						B) Doctor  
					</section>

					<section data-markdown>
						[The Unreasonable Effectiveness of Recurrent Neural Networks - Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
						
						![ho](http://karpathy.github.io/assets/rnn/pane1.png)

					</section>
					

					<section data-markdown>
						## Natural Language Processing


						AWD-LSTM - Highlight inputs via Sequential Jacobian.


						[Transformer, Sparse Transformer - Visualizing Attention.](http://jalammar.github.io/illustrated-transformer/)
					</section>
					<section data-markdown>
						Cancer Detection

						![](images/breast_cancer.png)
					</section>
					 <section>
					 	<h2> Feature visualization for CNN filters
					 	<img src="images/filters.png">
					 </section>
				</section>
				<section data-markdown>
					Summary

					* Machine learning is a tool - hone it, use it safely.
					* Do your best to inspect and validate your models.
					* Think about the end user and those affected by your work.
				</section>
				
				<section>
					<h2> Chatbot for voice conversation with book characters. </h2>
					<p> Laura Tolosi & Ariel Lubonja </p>
					<p> 13th July 2020</p>
				</section>
				<section id="themes">
					<h2>Themes</h2>
					<p>
						Reveal.js comes with a few themes built in: <br>
						<a href="?#/themes">Default</a> -
						<a href="?theme=sky#/themes">Sky</a> -
						<a href="?theme=beige#/themes">Beige</a> -
						<a href="?theme=simple#/themes">Simple</a> -
						<a href="?theme=serif#/themes">Serif</a> -
						<a href="?theme=night#/themes">Night</a> <br>
						<a href="?theme=moon#/themes">Moon</a> -
						<a href="?theme=solarized#/themes">Solarized</a>
					</p>
					<p>
						<small>
							* Theme demos are loaded after the presentation which leads to flicker. In production you should load your theme in the <code>&lt;head&gt;</code> using a <code>&lt;link&gt;</code>.
						</small>
					</p>
				</section>
				<section data-markdown>
					## Hat-trick: Thank you DEV.BG!

					1. Advances in Language Modeling: Text Generation for Fun and ___
					2. Implicit Data: Video Game Recommendations
					3. ML for Humans: Interpretability, Interactivity & Bias
				</section>


				
				
			</div>

		</div>

		<script src="bower_components/reveal-js/lib/js/head.min.js"></script>
		<script src="bower_components/reveal-js/js/reveal.min.js"></script>

		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

				// Parallax scrolling
				// parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
				// parallaxBackgroundSize: '2100px 900px',

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'bower_components/reveal-js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'bower_components/reveal-js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'bower_components/reveal-js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'bower_components/reveal-js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'bower_components/reveal-js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: 'bower_components/reveal-js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: 'bower_components/reveal-js/plugin/math/math.js', async: true }
				]
			});

		</script>

	</body>
</html>
